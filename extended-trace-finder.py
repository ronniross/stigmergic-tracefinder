# -*- coding: utf-8 -*-
"""extended-trace-finder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k0phzYzo-7OSbN9UXYr6RCWpIzE1i2jb

Extended Stigmergic Search

Because databases like OpenAlex and Semantic Scholar have *millions* of results, I have added a `max_batches = 10` (300 papers) safety limit to these scripts. Without this, fetching 50,000 papers at 5-second intervals would lock up your Colab notebook for 70 hours! You can easily change this number in the parameters.

*(Note: CORE is the only one that requires a free API key to function. I have marked where to put it in that specific cell).*
"""

# @title 1. Cell 1: bioRxiv / medRxiv (via Crossref)

import requests
import time
from IPython.display import Markdown, display

keywords_input = "animal communication"
batch_size = 30
max_batches = 10 # Set to a higher number if you want more than 300 results

def search_biorxiv():
    words = [w.strip().lower() for w in keywords_input.replace(",", " ").replace("_", " ").split() if w.strip()]
    query = "+".join(words)
    print(f"Searching bioRxiv for: {' AND '.join(words)}...\n")

    base_url = "https://api.crossref.org/works"
    all_results = []

    for batch_number in range(1, max_batches + 1):
        offset = (batch_number - 1) * batch_size
        # prefix:10.1101 restricts the search entirely to bioRxiv
        params = {"query": query, "filter": "prefix:10.1101", "rows": batch_size, "offset": offset}

        try:
            response = requests.get(base_url, params=params).json()
            items = response.get("message", {}).get("items", [])
        except Exception as e:
            print("Error reaching API.")
            break

        if not items:
            print("No more results found.")
            break

        print(f"\n--- BATCH {batch_number} (bioRxiv) ---")
        for i, res in enumerate(items, 1):
            title = res.get("title", ["Unknown Title"])[0]
            year = res.get("published", {}).get("date-parts", [[None]])[0][0] or "Unknown Year"
            print(f"{(batch_number - 1) * batch_size + i}. {title} ({year})")

            # Format authors
            authors_list = res.get("author", [])
            authors = ", ".join([f"{a.get('given', '')} {a.get('family', '')}".strip() for a in authors_list]) or "Unknown Authors"

            all_results.append({"title": title, "year": year, "authors": authors, "url": res.get("URL", "")})

        if len(items) < batch_size:
            break

        print("\nWaiting 5 seconds...")
        time.sleep(5)

    if all_results:
        md_content = f"### bioRxiv Results ({len(all_results)} fetched)\n\n"
        for r in all_results:
            md_content += f"* **[{r['title']}]({r['url']})** ({r['year']}) <br> ðŸ‘¤ *{r['authors']}*\n\n"
        display(Markdown(md_content))

search_biorxiv()

# @title 2. PMC Trace-Finder (Life Sciences & Biology.)
import requests
import time
from IPython.display import Markdown, display

keywords_input = "animal communication"
batch_size = 30
max_batches = 10

def search_europe_pmc():
    words = [w.strip().lower() for w in keywords_input.replace(",", " ").replace("_", " ").split() if w.strip()]
    query = " AND ".join(words) + " AND OPEN_ACCESS:Y"
    print(f"Searching Europe PMC for: {query}...\n")

    base_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
    all_results = []
    cursor = "*"

    for batch_number in range(1, max_batches + 1):
        params = {"query": query, "format": "json", "pageSize": batch_size, "cursorMark": cursor}

        try:
            response = requests.get(base_url, params=params).json()
            items = response.get("resultList", {}).get("result", [])
            cursor = response.get("nextCursorMark")
        except:
            print("Error reaching API.")
            break

        if not items:
            break

        print(f"\n--- BATCH {batch_number} (Europe PMC) ---")
        for i, res in enumerate(items, 1):
            title = res.get("title", "Unknown Title")
            year = res.get("pubYear", "Unknown Year")
            print(f"{(batch_number - 1) * batch_size + i}. {title} ({year})")

            authors = res.get("authorString", "Unknown Authors")
            url = f"https://doi.org/{res.get('doi')}" if res.get("doi") else f"https://europepmc.org/article/MED/{res.get('pmid')}"

            all_results.append({"title": title, "year": year, "authors": authors, "url": url})

        if not cursor or len(items) < batch_size:
            break

        print("\nWaiting 5 seconds...")
        time.sleep(5)

    if all_results:
        md_content = f"### Europe PMC Results ({len(all_results)} fetched)\n\n"
        for r in all_results:
            md_content += f"* **[{r['title']}]({r['url']})** ({r['year']}) <br> ðŸ‘¤ *{r['authors']}*\n\n"
        display(Markdown(md_content))

search_europe_pmc()

# @title 3. OpenAlex Trace-Finder (Open-Source Aggregator).
import requests
import time
from IPython.display import Markdown, display

keywords_input = "animal communication"
batch_size = 30
max_batches = 10

def search_openalex():
    words = [w.strip().lower() for w in keywords_input.replace(",", " ").replace("_", " ").split() if w.strip()]
    query = " ".join(words)
    print(f"Searching OpenAlex for: {query}...\n")

    base_url = "https://api.openalex.org/works"
    all_results = []

    for batch_number in range(1, max_batches + 1):
        params = {"search": query, "filter": "is_oa:true", "per-page": batch_size, "page": batch_number}

        try:
            response = requests.get(base_url, params=params).json()
            items = response.get("results", [])
        except:
            print("Error reaching API.")
            break

        if not items:
            break

        print(f"\n--- BATCH {batch_number} (OpenAlex) ---")
        for i, res in enumerate(items, 1):
            title = res.get("title", "Unknown Title")
            year = res.get("publication_year", "Unknown Year")
            print(f"{(batch_number - 1) * batch_size + i}. {title} ({year})")

            authors = ", ".join([a.get("author", {}).get("display_name", "") for a in res.get("authorships", [])]) or "Unknown"
            url = res.get("doi") or res.get("id")

            all_results.append({"title": title, "year": year, "authors": authors, "url": url})

        if len(items) < batch_size:
            break

        print("\nWaiting 5 seconds...")
        time.sleep(5)

    if all_results:
        md_content = f"### OpenAlex Results ({len(all_results)} fetched)\n\n"
        for r in all_results:
            md_content += f"* **[{r['title']}]({r['url']})** ({r['year']}) <br> ðŸ‘¤ *{r['authors']}*\n\n"
        display(Markdown(md_content))

search_openalex()