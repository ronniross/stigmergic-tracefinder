# -*- coding: utf-8 -*-
"""arXiv-stigmergic-trace-finder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f-SY6H1Dd-sI3FRXCj0Knjdq2PPGXZpq
"""

# exact phrase matching (arXiv-stigmergic-trace-finder)
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "arxiv"])

import arxiv
import time
from IPython.display import Markdown, display

# --- Search Parameters ---
# Tip: "animal communication" searches the exact phrase.
# If you want more results (e.g., 600+), use "animal, communication"
# to search for papers containing both words anywhere in the text.
keywords_input = "animal communication"
batch_size = 30
# -------------------------

def search_arxiv_in_batches():
    # Clean up keywords, strip extra spaces, and force LOWERCASE to make it case-insensitive
    keywords = [kw.strip().lower() for kw in keywords_input.split(",") if kw.strip()]

    if not keywords:
        print("Please provide at least one keyword.")
        return

    # Build the query
    query = " AND ".join([f'all:"{kw}"' for kw in keywords])
    print(f"Searching arXiv for: {query}...\n")

    # Set up the arXiv client and search
    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=10000, # High limit to get all available papers
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    results_generator = client.results(search)

    batch_number = 1
    total_fetched = 0
    all_results = [] # Store everything here to merge at the end

    while True:
        # Fetch the next batch
        current_batch = []
        try:
            for _ in range(batch_size):
                current_batch.append(next(results_generator))
        except StopIteration:
            pass # Reached the end of available API results

        if not current_batch:
            if total_fetched == 0:
                print("No papers found for those keywords.")
            break

        total_fetched += len(current_batch)
        all_results.extend(current_batch) # Save to master list

        # --- Print Titles and Year for this batch ---
        print(f"\n--- BATCH {batch_number} (Results {(batch_number-1)*batch_size + 1} to {total_fetched}) ---")
        for i, res in enumerate(current_batch, 1):
            overall_index = (batch_number - 1) * batch_size + i
            year = res.published.year
            print(f"{overall_index}. {res.title} ({year})")

        # Check if we hit the end of the results
        if len(current_batch) < batch_size:
            print("\n" + "="*70)
            print("--- Reached the end of available results. ---")
            break

        print("\nWaiting 5 seconds before fetching the next batch...")
        time.sleep(5)
        batch_number += 1

    # --- FINAL STEP: Merge all batches into one Markdown list ---
    if all_results:
        print("\nGenerating final merged clickable list...\n")

        md_content = f"### All Clickable Results ({len(all_results)} total papers found)\n\n"
        for res in all_results:
            # Extract author names
            authors = ", ".join([author.name for author in res.authors])
            # Extract year
            year = res.published.year
            # Format: * [Title](URL) (Year) - *Authors*
            md_content += f"* **[{res.title}]({res.entry_id})** ({year}) <br> ðŸ‘¤ *{authors}*\n\n"

        # Display the complete merged markdown
        display(Markdown(md_content))

# Run the function
search_arxiv_in_batches()

# dynamic phrase matching (arXiv-stigmergic-trace-finder)
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "arxiv"])

import arxiv
import time
from IPython.display import Markdown, display

# --- Search Parameters ---
keywords_input = "animal communication"
batch_size = 30
# -------------------------

def search_arxiv_in_batches():
    # Replace commas and underscores with spaces, then split into individual words
    # This stops the API from doing strict "exact phrase" matches and instead acts like the website
    raw_words = keywords_input.replace(",", " ").replace("_", " ").split()
    words = [w.strip().lower() for w in raw_words if w.strip()]

    if not words:
        print("Please provide at least one keyword.")
        return

    # Build the query: e.g., all:animal AND all:communication
    query = " AND ".join([f"all:{w}" for w in words])
    print(f"Searching arXiv broadly for: {query}...\n")

    # Set up the arXiv client and search
    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=10000, # High limit to get all available papers
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    results_generator = client.results(search)

    batch_number = 1
    total_fetched = 0
    all_results = [] # Store everything here to merge at the end

    while True:
        # Fetch the next batch
        current_batch = []
        try:
            for _ in range(batch_size):
                current_batch.append(next(results_generator))
        except StopIteration:
            pass # Reached the end of available API results

        if not current_batch:
            if total_fetched == 0:
                print("No papers found for those keywords.")
            break

        total_fetched += len(current_batch)
        all_results.extend(current_batch) # Save to master list

        # --- Print Titles and Year for this batch ---
        print(f"\n--- BATCH {batch_number} (Results {(batch_number-1)*batch_size + 1} to {total_fetched}) ---")
        for i, res in enumerate(current_batch, 1):
            overall_index = (batch_number - 1) * batch_size + i
            year = res.published.year
            print(f"{overall_index}. {res.title} ({year})")

        # Check if we hit the end of the results
        if len(current_batch) < batch_size:
            print("\n" + "="*70)
            print("--- Reached the end of available results. ---")
            break

        print("\nWaiting 5 seconds before fetching the next batch...")
        time.sleep(5)
        batch_number += 1

    # --- FINAL STEP: Merge all batches into one Markdown list ---
    if all_results:
        print("\n" + "="*70)
        print(f"Generating final merged clickable list for all {len(all_results)} papers...\n")

        md_content = f"### All Clickable Results ({len(all_results)} total papers found)\n\n"
        for res in all_results:
            # Extract author names
            authors = ", ".join([author.name for author in res.authors])
            # Extract year
            year = res.published.year
            # Format: * [Title](URL) (Year) - *Authors*
            md_content += f"* **[{res.title}]({res.entry_id})** ({year}) <br> ðŸ‘¤ *{authors}*\n\n"

        # Display the complete merged markdown
        display(Markdown(md_content))

# Run the function
search_arxiv_in_batches()
